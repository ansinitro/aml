## Appendix D: Glossary of Technical Terms

*   **Average Marginal Effect (AME):** The average change in the predicted probability (or outcome) when a given regressor increases by one unit.
*   **Bagging (Bootstrap Aggregating):** A machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression.
*   **Boosting:** A machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones.
*   **Cross-Validation (k-Fold):** A resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called $k$ that refers to the number of groups that a given data sample is to be split into.
*   **Heteroscedasticity:** A condition in which the variance of the residual term, or error term, in a regression model varies widely.
*   **Hyperparameter Tuning:** The problem of choosing a set of optimal hyperparameters for a learning algorithm.
*   **Multicollinearity:** A phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.
*   **One-Hot Encoding:** A technique used to represent categorical variables as numerical values in a machine learning model.
*   **Overfitting:** The production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably.
*   **R-Squared ($R^2$):** A statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.
*   **Regularization ($L_1/L_2$):** A set of methods for reducing overfitting in machine learning models. Typically involves adding a penalty term to the error function.
*   **Residual:** The difference between the observed value of the dependent variable ($y$) and the predicted value ($\hat{y}$).
*   **RMSE (Root Mean Square Error):** A frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed.
